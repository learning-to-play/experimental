{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX implementation of Andrej Karpathy's [nanoGPT Colab](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filename = wget.download(url, '/tmp/tinyshakespeare.txt')\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "data = jnp.array(encode(text))\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "seed = 1701\n",
    "prng_key = random.key(seed)\n",
    "\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(prng_key, split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = random.randint(prng_key, (batch_size,), 0, len(data) - block_size)\n",
    "    x = jnp.stack([data[i:i + block_size] for i in ix])\n",
    "    y = jnp.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "prng_key, prng_subkey = random.split(prng_key)\n",
    "xb, yb = get_batch(prng_subkey, 'train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import optax\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, idx):\n",
    "        logits = nn.Embed(vocab_size, vocab_size)(idx)\n",
    "        return logits\n",
    "\n",
    "    def loss(self, embeddings, idx, labels):\n",
    "        # idx and labels are both (B, T) tensor of integers\n",
    "        logits = self.apply(embeddings, idx) # (B, T, C)\n",
    "        return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "\n",
    "    def generate(self, prng_key, embeddings, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits = self.apply(embeddings, idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            prng_key, prng_subkey = random.split(prng_key)\n",
    "            # sample from the distribution\n",
    "            idx_next = random.categorical(prng_key, logits) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = jnp.concatenate((idx, jnp.reshape(idx_next, (-1, 1))), axis=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel()\n",
    "prng_key, prng_subkey = random.split(prng_key)\n",
    "embeddings = m.init(prng_subkey, jnp.array(0))\n",
    "logits = m.apply(embeddings, xb)\n",
    "print(logits.shape)\n",
    "loss = m.loss(embeddings, xb, yb)\n",
    "print(loss)\n",
    "\n",
    "prng_key, prng_subkey = random.split(prng_key)\n",
    "print(decode(m.generate(prng_subkey, embeddings, idx=jnp.zeros((1, 1), dtype=int), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_loss = jax.grad(m.loss, argnums=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adamw(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "optimizer_state = optimizer.init(embeddings)\n",
    "\n",
    "for steps in range(100): # increase number of steps for good results...\n",
    "    prng_key, prng_subkey = random.split(prng_key)\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(prng_subkey, 'train')\n",
    "\n",
    "    grad = grad_loss(embeddings, xb, yb)\n",
    "    updates, optimizer_state = optimizer.update(grad, optimizer_state, embeddings)\n",
    "    embeddings = optax.apply_updates(embeddings, updates)\n",
    "\n",
    "loss = m.loss(embeddings, xb, yb)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng_key, prng_subkey = random.split(prng_key)\n",
    "print(decode(m.generate(prng_key, embeddings, idx = jnp.zeros((1, 1), dtype=int), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The mathentical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "a = jnp.tril(jnp.ones((3, 3)))\n",
    "a = a / jnp.sum(a, 1, keepdims=True)\n",
    "\n",
    "prng_key, prng_subkey = random.split(prng_key)\n",
    "b = random.randint(prng_subkey, (3, 2), 0, 10).astype(float)\n",
    "\n",
    "c = a @ b\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "prng_key, prng_subkey = random.split(prng_key)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = random.normal(prng_subkey, (B, T, C))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = jnp.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t + 1] # (t,C)\n",
    "        xbow = xbow.at[b, t].set(jnp.mean(xprev, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = jnp.tril(jnp.ones((T, T)))\n",
    "wei = wei / wei.sum(1, keepdims=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "jnp.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 3: use Softmax\n",
    "tril = jnp.tril(jnp.ones((T, T)))\n",
    "wei = jnp.zeros((T, T))\n",
    "wei = jnp.where(tril == 0, float('-inf'), wei)\n",
    "wei = nn.softmax(wei, axis=-1)\n",
    "xbow3 = wei @ x\n",
    "jnp.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 4: self-attention!\n",
    "prng_key, prng_subkey = random.split(prng_key)\n",
    "B, T, C = 4, 8, 32 # batch, time, channels\n",
    "x = random.normal(prng_subkey, (B, T, C))\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Dense(head_size, use_bias=False)\n",
    "query = nn.Dense(head_size, use_bias=False)\n",
    "value = nn.Dense(head_size, use_bias=False)\n",
    "# Initialize\n",
    "prng_key, prng_subkey = random.split(prng_key)\n",
    "key_vars = key.init(prng_subkey, x)\n",
    "query_vars = query.init(prng_subkey, x)\n",
    "value_vars = value.init(prng_subkey, x)\n",
    "k = key.apply(key_vars, x) # (B, T, 16)\n",
    "q = query.apply(query_vars, x) # (B, T, 16)\n",
    "wei = q @ jnp.transpose(k, (0, -1, -2)) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = jnp.tril(jnp.ones((T, T)))\n",
    "#wei = jnp.zeros((T, T))\n",
    "wei = jnp.where(tril==0, float('-inf'), wei)\n",
    "wei = nn.softmax(wei, axis=-1)\n",
    "\n",
    "v = value.apply(value_vars, x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full finished code, for reference\n",
    "You may want to refer directly to the git repo instead though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "import optax\n",
    "import wget\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "prng_key = random.key(1337)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filename = wget.download(url, '/tmp/tinyshakespeare.txt')\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = jnp.array(encode(text))\n",
    "n = int(0.9 * len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(prng_key, split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = random.randint(prng_key, (batch_size,), 0, len(data) - block_size)\n",
    "    x = jnp.stack([data[i:i + block_size] for i in ix])\n",
    "    y = jnp.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def estimate_loss(prng_key, params):\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = jnp.zeros((eval_iters,))\n",
    "        for k in range(eval_iters):\n",
    "            prng_key, prng_subkey = random.split(prng_key)\n",
    "            X, Y = get_batch(prng_subkey, split)\n",
    "            loss = model.loss(params, X, Y, training=split==\"train\")\n",
    "            losses = losses.at[k].set(loss)\n",
    "        out[split] = losses.mean()\n",
    "    return out\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    head_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool):\n",
    "        B, T, C = x.shape\n",
    "        k = nn.Dense(self.head_size, use_bias=False)(x)   # (B,T,C)\n",
    "        q = nn.Dense(self.head_size, use_bias=False)(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ jnp.transpose(k, (0, -1, -2)) * C ** -0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = jnp.where(jnp.tril(jnp.ones((T, T))) == 0, float('-inf'), wei) # (B, T, T)\n",
    "        wei = nn.softmax(wei, axis=-1) # (B, T, T)\n",
    "        wei = nn.Dropout(rate=dropout, deterministic=not training)(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = nn.Dense(self.head_size, use_bias=False)(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    num_heads: int\n",
    "    head_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool):\n",
    "        out = jnp.concatenate([Head(self.head_size)(x, training=training) for _ in range(self.num_heads)], axis=-1)\n",
    "        out = nn.Dense(n_embd)(out)\n",
    "        out = nn.Dropout(rate=dropout, deterministic=not training)(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool):\n",
    "        return nn.Sequential([\n",
    "            nn.Dense(4 * n_embd),\n",
    "            nn.relu,\n",
    "            nn.Dense(n_embd),\n",
    "            nn.Dropout(dropout, deterministic=not training),\n",
    "        ])(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    n_embd: int\n",
    "    n_head: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool):\n",
    "        head_size = self.n_embd // self.n_head\n",
    "        x = nn.LayerNorm()(x)\n",
    "        x = x + MultiHeadAttention(self.n_head, head_size)(x, training=training)\n",
    "        x = nn.LayerNorm()(x)\n",
    "        x = x + FeedForward()(x, training=training)\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, idx, training: bool):\n",
    "        _, T = idx.shape\n",
    "        # idx and labels are both (B, T) tensor of integers\n",
    "        tok_emb = nn.Embed(vocab_size, n_embd)(idx)\n",
    "        pos_emb = nn.Embed(block_size, n_embd)(jnp.arange(0, T))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = nn.Sequential([partial(Block(n_embd, n_head), training=training) for _ in range(n_layer)])(x) # (B, T, C)\n",
    "        x = nn.LayerNorm()(x) # (B, T, C)\n",
    "        logits = nn.Dense(vocab_size)(x) # (B, T, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def loss(self, params, idx, labels, training: bool):\n",
    "        logits = self.apply(params, idx, training=training) # (B, T, C)\n",
    "        return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "\n",
    "    def generate(self, prng_key, params, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits = self.apply(params, idx_cond, training=False)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            prng_key, prng_subkey = random.split(prng_key)\n",
    "            # sample from the distribution\n",
    "            idx_next = random.categorical(prng_subkey, logits) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = jnp.concatenate((idx, jnp.reshape(idx_next, (-1, 1))), axis=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "# print the number of parameters in the model\n",
    "prng_key, prng_subkey = random.split(prng_key)\n",
    "print(model.tabulate(\n",
    "    prng_subkey,\n",
    "    jnp.zeros((batch_size, block_size), dtype=int),\n",
    "    training=True,\n",
    "    compute_flops=True,\n",
    "    compute_vjp_flops=True,\n",
    "))\n",
    "# print(jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(params)))\n",
    "\n",
    "grad_loss = jax.grad(model.loss, argnums=0)\n",
    "\n",
    "optimizer = optax.adamw(learning_rate=1e-3)\n",
    "params = model.init(prng_subkey, xb, training=True)\n",
    "optimizer_state = optimizer.init(params)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        prng_key, prng_subkey = random.split(prng_key)\n",
    "        losses = estimate_loss(prng_subkey, params)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    prng_key, prng_subkey = random.split(prng_key)\n",
    "    xb, yb = get_batch(prng_subkey, 'train')\n",
    "\n",
    "    grad = grad_loss(params, xb, yb, training=True)\n",
    "    updates, optimizer_state = optimizer.update(grad, optimizer_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "\n",
    "# generate from the model\n",
    "prng_key, prng_subkey = random.split(prng_key)\n",
    "context = jnp.zeros((1, 1), dtype=int)\n",
    "print(decode(model.generate(prng_subkey, params, context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. Should JAX equivalent of `register_buffer` be used for `tril`?\n",
    "1. Add running on device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: KV cache for single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4096\n",
    "batch_size = 128\n",
    "block_size = 2048\n",
    "n_embd = 2048\n",
    "n_head = 1\n",
    "\n",
    "head_size = n_embd // n_head\n",
    "head_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKV(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training=True, kvcache=None):\n",
    "        if not training:\n",
    "            x = x[:, -1:, :]\n",
    "\n",
    "        q = nn.Dense(head_size, use_bias=False)(x)\n",
    "        k = nn.Dense(head_size, use_bias=False)(x)\n",
    "        v = nn.Dense(head_size, use_bias=False)(x)\n",
    "\n",
    "        if not training:\n",
    "            if kvcache:\n",
    "                k = jnp.concatenate((kvcache['k'], k), axis=1)[:, -block_size:, :]\n",
    "                v = jnp.concatenate((kvcache['v'], v), axis=1)[:, -block_size:, :]\n",
    "            kvcache['k'] = k\n",
    "            kvcache['v'] = v\n",
    "\n",
    "        return q, k, v\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, context, training=True, kvcache=None):\n",
    "        context_block = context[:, -block_size:]\n",
    "        pos = jnp.arange(0, context_block.shape[1])\n",
    "\n",
    "        tok_embed = nn.Embed(vocab_size, n_embd)(context_block)\n",
    "        pos_embed = nn.Embed(block_size, n_embd)(pos)\n",
    "        x = tok_embed + pos_embed\n",
    "\n",
    "        if not training:\n",
    "            if 'qkv' not in kvcache:\n",
    "                kvcache['qkv'] = {}\n",
    "            kvcache = kvcache['qkv']\n",
    "        q, k, v = QKV()(x, training=training, kvcache=kvcache)\n",
    "\n",
    "        B, T, C = x.shape\n",
    "        wei = q @ jnp.transpose(k, (0, -1, -2)) * C ** -0.5\n",
    "        wei = nn.softmax(wei, axis=-1)\n",
    "        out = wei @ v\n",
    "\n",
    "        logits = nn.Dense(vocab_size)(out)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def generate(self, prng_key, params, context, max_new_tokens=1):\n",
    "        kvcache = {}\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.apply(params, context, training=False, kvcache=kvcache)\n",
    "            prng_key, prng_subkey = random.split(prng_key)\n",
    "            context_next = random.categorical(prng_subkey, logits)\n",
    "            context = jnp.concatenate((context, jnp.reshape(context_next, (-1, 1))), axis=1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transformer = Transformer()\n",
    "params = transformer.init(prng_key, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = transformer.generate(random.key(0), params, jnp.zeros((1, 1), dtype=int), 10)\n",
    "context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_12_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
